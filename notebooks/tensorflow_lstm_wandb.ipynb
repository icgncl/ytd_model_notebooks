{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "V-Vmv2tnqvrt"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import datetime\n",
        "%matplotlib inline\n",
        "import wandb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TguFmk71qvrv"
      },
      "source": [
        "### Step 0. Loading dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "run = wandb.init()\n",
        "artifact = run.use_artifact('ytdteam/ytd-cassandra-forecast/meta-stock-price:v0', type='raw_data')\n",
        "artifact_dir = artifact.download()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "no9NmFNmqvrz"
      },
      "outputs": [],
      "source": [
        "stock = pd.read_csv('artifacts/meta-stock-price-v0/meta.us.txt')\n",
        "stock.columns = ['TICKER','PER','DATE','TIME','OPEN','HIGH','LOW','CLOSE','VOL','OPENINT']\n",
        "stock['TIME'] = pd.to_datetime(stock['TIME'],format='%H%M%S').dt.time\n",
        "stock['DATE'] = pd.to_datetime(stock['DATE'],format='%Y%m%d').dt.date"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "id": "TL0w6hVMrMgN",
        "outputId": "ec644600-aa08-4ee4-b0c0-1d3be06a24ae"
      },
      "outputs": [],
      "source": [
        "\n",
        "print(stock.isnull().sum())                     \n",
        "stock.groupby(by=[\"TIME\"]).count()\n",
        "stock = stock[(stock.TIME!=datetime.time(15,0,0))]\n",
        "stock['Date-time'] = pd.to_datetime(stock.DATE.astype(str) + ' ' + stock.TIME.astype(str))\n",
        "stock=stock.set_index(pd.DatetimeIndex(stock['Date-time'].values))\n",
        "stock.head()                        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "83j5Dbpqqvr2"
      },
      "outputs": [],
      "source": [
        "data_to_use = stock['CLOSE'].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4DwEqcF_qvr3",
        "outputId": "9dc3a524-6c9e-438d-a323-d1eed47f213a"
      },
      "outputs": [],
      "source": [
        "print('Total number of data points in the dataset: {}'.format(len(data_to_use)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWwk0aP7qvr5"
      },
      "source": [
        "### Step 1. Data preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dMucVcC0qvr6"
      },
      "source": [
        "#### Step 1.1 Scaling data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "iASKlg9dqvr8"
      },
      "outputs": [],
      "source": [
        "scaler = StandardScaler()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4f7rRcYTqvr9"
      },
      "outputs": [],
      "source": [
        "scaled_dataset = scaler.fit_transform(data_to_use.reshape(-1, 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        },
        "id": "SesanYmJqvr_",
        "outputId": "441dc8e6-4402-417a-d43f-15227e9afe65",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12,7), frameon=False, facecolor='brown', edgecolor='blue')\n",
        "plt.title('Scaled META Stock Price Change from December 2021 to October 2022')\n",
        "plt.ylabel('Scaled value of stocks')\n",
        "plt.plot(scaled_dataset, label='Stocks data')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "3yIvqaGrqvsA"
      },
      "outputs": [],
      "source": [
        "def window_data(data, window_size):\n",
        "    X = []\n",
        "    y = []\n",
        "    \n",
        "    i = 0\n",
        "    while (i + window_size) <= len(data) - 1:\n",
        "        X.append(data[i:i+window_size])\n",
        "        y.append(data[i+window_size])\n",
        "        \n",
        "        i += 1\n",
        "    assert len(X) ==  len(y)\n",
        "    return X, y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6xH3uxaqvsB"
      },
      "source": [
        "#### Step 1.2 Windowing the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GnV715rmqvsB"
      },
      "outputs": [],
      "source": [
        "X, y = window_data(scaled_dataset, 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ieyC5kGVqvsC"
      },
      "source": [
        "#### Step 1.3 Creating Train and Test sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TjTujctLqvsD",
        "outputId": "a50661a5-cfbf-4ac0-ff05-45105bda3df4"
      },
      "outputs": [],
      "source": [
        "X_train  = np.array(X[:1155])\n",
        "y_train = np.array(y[:1155])\n",
        "\n",
        "X_test = np.array(X[1155:])\n",
        "y_test = np.array(y[1155:])\n",
        "\n",
        "print(\"X_train size: {}\".format(X_train.shape))\n",
        "print(\"y_train size: {}\".format(y_train.shape))\n",
        "print(\"X_test size: {}\".format(X_test.shape))\n",
        "print(\"y_test size: {}\".format(y_test.shape))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BrfBWgpzFAfJ"
      },
      "outputs": [],
      "source": [
        "def calculate_rmse(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Calculate the Root Mean Squared Error (RMSE)  \n",
        "    \"\"\"\n",
        "    rmse = np.sqrt(np.mean((y_true-y_pred)**2))                   \n",
        "    return rmse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yLKNBfuRLE2d"
      },
      "outputs": [],
      "source": [
        "def calculate_mape(y_true, y_pred): \n",
        "    \"\"\"\n",
        "    Calculate the Mean Absolute Percentage Error (MAPE) %\n",
        "    \"\"\"\n",
        "    y_pred, y_true = np.array(y_pred), np.array(y_true)    \n",
        "    mape = np.mean(np.abs((y_true-y_pred) / y_true))*100    \n",
        "    return mape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tD1uPSarqvsD"
      },
      "source": [
        "### Let's create the RNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "7_xwg2dJqvsE"
      },
      "outputs": [],
      "source": [
        "def LSTM_cell(hidden_layer_size, batch_size,number_of_layers, dropout=True, dropout_rate=0.8):\n",
        "    \n",
        "    layer = tf.compat.v1.nn.rnn_cell.BasicLSTMCell(hidden_layer_size)\n",
        "    \n",
        "    if dropout:\n",
        "        layer = tf.compat.v1.nn.rnn_cell.DropoutWrapper(layer, output_keep_prob=dropout_rate)\n",
        "        \n",
        "    cell = tf.compat.v1.nn.rnn_cell.MultiRNNCell([layer]*number_of_layers)\n",
        "    \n",
        "    init_state = cell.zero_state(batch_size, tf.float32)\n",
        "    \n",
        "    return cell, init_state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "KBn3vRWwqvsE"
      },
      "outputs": [],
      "source": [
        "def output_layer(lstm_output, in_size, out_size):\n",
        "    \n",
        "    x = lstm_output[:, -1, :]\n",
        "    print(x)\n",
        "    weights = tf.Variable(tf.compat.v1.random.truncated_normal([in_size, out_size], stddev=0.05), name='output_layer_weights')\n",
        "    bias = tf.Variable(tf.zeros([out_size]), name='output_layer_bias')\n",
        "    \n",
        "    output = tf.matmul(x, weights) + bias\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "U6aqRQuyqvsF"
      },
      "outputs": [],
      "source": [
        "def opt_loss(logits, targets, learning_rate, grad_clip_margin, batch_size):\n",
        "    \n",
        "    losses = []\n",
        "    for i in range(targets.get_shape()[0]):\n",
        "        losses.append([(tf.pow(logits[i] - targets[i], 2))])\n",
        "        \n",
        "    loss = tf.reduce_sum(losses)/(2*batch_size)\n",
        "    \n",
        "    #Cliping the gradient loss\n",
        "    gradients = tf.gradients(loss, tf.compat.v1.trainable_variables())\n",
        "    clipper_, _ = tf.clip_by_global_norm(gradients, grad_clip_margin)\n",
        "    optimizer = tf.optimizers.Adam(learning_rate)\n",
        "    train_optimizer = optimizer.apply_gradients(zip(gradients, tf.compat.v1.trainable_variables()))\n",
        "    return loss, train_optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TX93N2DYqvsF"
      },
      "outputs": [],
      "source": [
        "class StockPredictionRNN(object):\n",
        "    \n",
        "    def __init__(self, learning_rate=0.0005, batch_size=2, hidden_layer_size=512, number_of_layers=1, \n",
        "                 dropout=True, dropout_rate=0.8, number_of_classes=1, gradient_clip_margin=4, window_size=1):\n",
        "    \n",
        "        tf.compat.v1.disable_eager_execution()\n",
        "        self.inputs = tf.compat.v1.placeholder(tf.float32, [batch_size, window_size, 1], name='input_data')\n",
        "        self.targets = tf.compat.v1.placeholder(tf.float32, [batch_size, 1], name='targets')\n",
        "\n",
        "        cell, init_state = LSTM_cell(hidden_layer_size, batch_size, number_of_layers, dropout, dropout_rate)\n",
        "\n",
        "        outputs, states = tf.compat.v1.nn.dynamic_rnn(cell, self.inputs, initial_state=init_state)\n",
        "\n",
        "        self.logits = output_layer(outputs, hidden_layer_size, number_of_classes)\n",
        "\n",
        "        self.loss, self.opt = opt_loss(self.logits, self.targets, learning_rate, gradient_clip_margin, batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Weight and Biases Experiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sweep_config = {\n",
        "    'method': 'random',\n",
        "    'metric': {'goal': 'minimize', 'name': 'mape'},\n",
        "    'parameters': {\n",
        "        'batch_size': {\n",
        "            'distribution': 'q_log_uniform_values',\n",
        "            'max': 64,\n",
        "            'min': 2,\n",
        "            'q': 2\n",
        "            },\n",
        "        'dropout_rate': {'values': [0.3, 0.4, 0.5, 0.6, 0.7, 0.8]},\n",
        "        'epochs': {'values': [30, 40, 50, 60, 70, 80]},\n",
        "        'learning_rate': {\n",
        "            'distribution': 'uniform',\n",
        "            'max': 0.0010,\n",
        "            'min': 0\n",
        "        },\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sweep_id = wandb.sweep(sweep_config, project=\"tensorflow-lstm-sweep\", entity=\"ytdteam\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train(config=None):\n",
        "    # Initialize a new wandb run\n",
        "    with wandb.init(config=config):\n",
        "        # If called by wandb.agent, as below,\n",
        "        # this config will be set by Sweep Controller\n",
        "        config = wandb.config\n",
        "        tf.compat.v1.reset_default_graph()\n",
        "        model = StockPredictionRNN(\n",
        "            learning_rate = config.learning_rate,\n",
        "            batch_size = config.batch_size,\n",
        "            dropout_rate = config.dropout_rate,\n",
        "        )\n",
        "        session = tf.compat.v1.Session()\n",
        "        session.run(tf.compat.v1.global_variables_initializer())\n",
        "\n",
        "        for i in range(config.epochs):\n",
        "            traind_scores = []\n",
        "            ii = 0\n",
        "            epoch_loss = []\n",
        "            while(ii + config.batch_size) <= len(X_train):\n",
        "                X_batch = X_train[ii:ii+config.batch_size]\n",
        "                y_batch = y_train[ii:ii+config.batch_size]\n",
        "                \n",
        "                o, c, _ = session.run([model.logits, model.loss, model.opt], feed_dict={model.inputs:X_batch, model.targets:y_batch})\n",
        "                \n",
        "                epoch_loss.append(c)\n",
        "                traind_scores.append(o)\n",
        "                ii += config.batch_size\n",
        "            # Evaluate\n",
        "            sup =[]\n",
        "            for ii in range(len(traind_scores)):\n",
        "                for jj in range(len(traind_scores[ii])):\n",
        "                    sup.append(traind_scores[ii][jj])\n",
        "            tests = []\n",
        "            ii = 0\n",
        "            while ii+config.batch_size <= len(X_test):                \n",
        "                o = session.run([model.logits], feed_dict={model.inputs:X_test[ii:ii+config.batch_size]})\n",
        "                ii += config.batch_size\n",
        "                tests.append(o)\n",
        "            tests_new = []\n",
        "            for ii in range(len(tests)):\n",
        "                for jj in range(len(tests[ii][0])):\n",
        "                    tests_new.append(tests[ii][0][jj])\n",
        "            test_results = []\n",
        "            for ii in range(1446):\n",
        "                if ii >= 1156:\n",
        "                    test_results.append(tests_new[ii-1156])\n",
        "                else:\n",
        "                    test_results.append(None)\n",
        "            # Plot\n",
        "            # fig = plt.figure(figsize=(16, 7))\n",
        "            # plt.plot(scaled_dataset, label='Original data')\n",
        "            # plt.plot(sup, label='Training data')\n",
        "            # plt.plot(test_results, label='Testing data')\n",
        "            # plt.legend()\n",
        "            # plt.show()\n",
        "            # plt.close()\n",
        "            # Metrics\n",
        "            y_true = np.array(scaled_dataset[1158:1446])\n",
        "            y_pred = np.array(test_results[1158:1446])\n",
        "            mape_lstm = calculate_mape(y_true, y_pred)\n",
        "            rmse_lstm = calculate_rmse(scaler.inverse_transform(y_true), scaler.inverse_transform(y_pred))\n",
        "            # Wandb\n",
        "            wandb.log({\"loss\": sum(epoch_loss)/len(epoch_loss), \"epoch\": i, \"mape\": mape_lstm, \"rmse\": rmse_lstm})\n",
        "            print('Epoch {}/{}'.format(i, config.epochs), ' Current loss: {}'.format(np.mean(epoch_loss)))\n",
        "            session.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "wandb.agent(sweep_id, train, count=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLNsUyEVqvsQ"
      },
      "source": [
        "### References\n",
        "https://modelzoo.co/model/tesla-stocks-prediction  \n",
        "https://github.com/lucko515/tesla-stocks-prediction  "
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
